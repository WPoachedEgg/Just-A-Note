---
layout: post
title:  "Sorting"
date:   2022-09-08 08:30:00
categories: Algorithm
tags: [Sorting, Basic]
---

* [QuickSort](2022-09-14-Sorting.md#QuickSort)
  * [Vocabulary](2022-09-14-Sorting.md#Vocabulary)
  * [Definition](2022-09-14-Sorting.md#Definition)
* [MergeSort](2022-09-14-Sorting.md#mergeSort)
  * [Assertion](2022-09-14-Sorting.md#assertion)
  * [Merge Sort Implementation](2022-09-14-Sorting.md#Merge-Sort-Implementation)
  * [Empirical Analysis](2022-09-14-Sorting.md#Empirical-Analysis)
  * [Prcatical Improvments](2022-09-14-Sorting.md#Prcatical-Improvments)
* [HeapSort](2022-09-14-Sorting.md#HeapSort)
  * [somthing](2022-09-14-Sorting.md#somthing)
* [RadixSort](2022-09-14-Sorting.md#RadixSort)
  * [somthing](2022-09-14-Sorting.md#somthing)
* [Priority Queues](2022-09-14-Sorting.md#priority-queues)
  * [Vocabulary](2022-09-14-Sorting.md#Vocabulary)
  * [Definition](2022-09-14-Sorting.md#Definition) <!-- more -->

## Elementary Sorts

### Rules of the game. 

Our primary concern is algorithms for rearranging arrays of items where each item contains a key. The objective is to rearrange the items such that their keys are in ascending order. In Java, the abstract notion of a key is captured in a built-in mechanism—the Comparable interface. With but a few exceptions, our sort code refers to the data only through two operations: the method less() that compares objects and the method exch() that exchanges them.
``` java
private static boolean less(Comparable v, Comparable w) {
   return (v.compareTo(w) < 0);
}

private static void exch(Comparable[] a, int i, int j) {
   Comparable swap = a[i];
   a[i] = a[j];
   a[j] = swap;
} 
```

* Sorting cost model. 

When studying sorting algorithms, we count compares and exchanges. For algorithms that do not use exchanges, we count array accesses.
Extra memory. The sorting algorithms we consider divide into two basic types: those that sort in place (no extra memory except perhaps for a small function-call stack or a constant number of instance variables), and those that need enough extra memory to hold another copy of the array to be sorted.

* Types of data. 
Our sort code is effective for any type of data that implements Java's * Comparable interface. This means that there is a method compareTo() for which v.compareTo(w) returns an integer that is negative, zero, or positive when v < w, v = w, or v > w, respectively. The method must implement a total order:
  * Reflexive: for all v, v = v.
  * Antisymmetric: for all v and w, if (v < w) then (w > v); and if (v = w) then (w = v)  
  * Transitive: for all v, w, and x, if (v ≤ w) and (w ≤ x), then v ≤ x.
  In addition, v.compareTo(w) must throw an exception if v and w are of incompatible types or if either is null.

### Selection sort. 

One of the simplest sorting algorithms works as follows: First, find the smallest item in the array, and exchange it with the first entry. Then, find the next smallest item and exchange it with the second entry. Continue in this way until the entire array is sorted. This method is called selection sort because it works by repeatedly selecting the smallest remaining item. 
**Proposition.** Selection sort uses ~n2/2 compares and n exchanges to sort an array of length n.

### Insertion sort. 

The algorithm that people often use to sort bridge hands is to consider the cards one at a time, inserting each into its proper place among those already considered (keeping them sorted). In a computer implementation, we need to make space for the current item by moving larger items one position to the right, before inserting the current item into the vacated position.

**Proposition.** For randomly ordered arrays of length N with with distinct keys, insertion sort uses ~N2/4 compares and ~N2/4 exchanges on the average. The worst case is ~ N2/2 compares and ~ N2/2 exchanges and the best case is N-1 compares and 0 exchanges.
Insertion sort works well for certain types of nonrandom arrays that often arise in practice, even if they are huge. An inversion is a pair of keys that are out of order in the array. For instance, E X A M P L E has 11 inversions: E-A, X-A, X-M, X-P, X-L, X-E, M-L, M-E, P-L, P-E, and L-E. If the number of inversions in an array is less than a constant multiple of the array size, we say that the array is partially sorted.

Proposition. The number of exchanges used by insertion sort is equal to the number of inversions in the array, and the number of compares is at least equal to the number of inversions and at most equal to the number of inversions plus the array size.
Property. For randomly ordered arrays of distinct values, the running times of insertion sort and selection sort are quadratic and within a small constant factor of one another.
SortCompare.java uses the sort() methods in the classes named as command-line arguments to perform the given number of experiments (sorting arrays of the given size) and prints the ratio of the observed running times of the algorithms.

Visualizing sorting algorithms. We use a simple visual representation to help describe the properties of sorting algorithms. We use vertical bars, to be sorted by their heights. SelectionBars.java and InsertionBars.java produce these visualizations.
Visualization of selection sort and insertion sort
Shellsort. Shellsort is a simple extension of insertion sort that gains speed by allowing exchanges of entries that are far apart, to produce partially sorted arrays that can be efficiently sorted, eventually by insertion sort. The idea is to rearrange the array to give it the property that taking every hth entry (starting anywhere) yields a sorted sequence. Such an array is said to be h-sorted.


## MergeSort

**Basic Logic**

  * Divide array into two halves.
  * Recursively sort each half.
  * Merge two halves.

**Goal.** Given two sorted subarrays a[lo] to a[mid] and a[mid+1] to a[hi],
replace with sorted subarray a[lo] to a[hi].
  ![mergesort-overview](/assets/mergesort-overview.png)

  ``` java
    private static void merge(Comparable[] a, Comparable[] aux, int lo, int mid, int hi)
    {
      assert isSorted(a, lo, mid); // precondition: a[lo..mid] sorted
      assert isSorted(a, mid+1, hi); // precondition: a[mid+1..hi] sorted
      for (int k = lo; k <= hi; k++)
        aux[k] = a[k];
        
      int i = lo, j = mid+1;
      for (int k = lo; k <= hi; k++)
      {
        if (i > mid) a[k] = aux[j++];
        else if (j > hi) a[k] = aux[i++];
        else if (less(aux[j], aux[i])) a[k] = aux[j++];
        else a[k] = aux[i++];
      }
      assert isSorted(a, lo, hi); // postcondition: a[lo..hi] sorted
    }  
  ```
### Assertion

**Assertion.** Statement to test assumptions about your program.
* Helps detect logic bugs.
* Documents code

**Java assert statement.** Throws exception unless boolean condition is true.
  ```java
    assert isSorted(a, lo, hi);
  ```
**Can enable or disable at runtime.** ⇒ No cost in production code.   
  ```java
    java -ea MyProgram // enable assertions
    java -da MyProgram // disable assertions (default)
  ```
**Best practices.** Use assertions to check internal invariants;
assume assertions will be disabled in production code.

### Merge Sort Implementation

  ```java
    private static void sort(Comparable[] a, Comparable[] aux, int lo, int hi)
    {
      if (hi <= lo) return;
      int mid = lo + (hi - lo) / 2;
      sort(a, aux, lo, mid);
      sort(a, aux, mid+1, hi);
      merge(a, aux, lo, mid, hi);
    }
    public static void sort(Comparable[] a)
    {
      aux = new Comparable[a.length];
      sort(a, aux, 0, a.length - 1);
    }
  ```

### Empirical Analysis

1. number of compares and array accesses

#### Proposition. Bottom-up mergesort uses between 1/2 N lg N and N lg N compares and at most 6 N lg N array accesses to sort any array of length N.

**Proof Sketch:**
  The number of compares C(N) and array accesses A (N)
  to mergesort an array of size N satisfy the recurrences:

    C(N) ≤ C(⎡N / 2⎤) + C(⎣N/ 2⎦) + N    for N > 1, with C(1) = 0.

          left half  + right half + merge

    A (N) ≤ A (⎡N / 2⎤) + A (⎣N/ 2⎦) + 6N   for N > 1, with A (1) = 0.

  We solve the recurrence when N is a power of 2.
  * D (N) = 2 D (N/ 2) + N, for N > 1, with D (1) = 0

#### Proposition. If D (N) satisfies D (N) = 2 D (N / 2) + N for N > 1, with D (1) = 0, then D (N) = N lg N.
**Proof 1: proof by picture**
  ![mergesort-pf1](/assets/mergesort-pf1.png)

**Proof 2: proof by expansion**  [assuming N is a power of 2]

      D (N) = 2 D (N/2) + N        
      D (N) / N = 2 D (N/2) / N + 1
                = D (N/2) / (N/2) + 1
                = D (N/4) / (N/4) + 1 + 1
                = D (N/8) / (N/8) + 1 + 1 + 1
                  . . .
                = D (N/N) / (N/N) + 1 + 1 + ... + 1
                = lg N

**Proof 3: proof by induction**  [assuming N is a power of 2]

  * Base case: N = 1.
  * Inductive hypothesis: D (N) = N lg N.
  * Goal: show that D (2N) = (2N) lg (2N).

        D(2N) = 2 D(N) + 2N
              = 2 N lg N + 2N
              = 2 N (lg (2N) – 1) + 2N
              = 2 N lg (2N) 


#### Proposition. No compare-based sorting algorithm can guarantee to sort N items with fewer than lg(N!) ~ N lg N compares.

#### Proposition. Mergesort is an asymptotically optimal compare-based sorting algorithm. That is, both the number of compares used by mergesort in the worst case and the minimum number of compares that any compare-based sorting algorithm can guarantee are ~N lg N.


2. Memory analysis

**Proposition.** Mergesort uses extra space proportional to N.

  Pf. The array aux[] needs to be of size N for the last merge.

  Def. A sorting algorithm is in-place if it uses ≤ c log N extra memory.

  Ex. Insertion sort, selection sort, shellsort.

### Prcatical Improvments

1. Use insertion sort for small subarrays.
  * Mergesort has too much overhead for tiny subarrays.
  * Cutoff to insertion sort for ≈ 7 items.

2. Stop if already sorted.
  * Is biggest item in first half ≤ smallest item in second half?
  * Helps for partially-ordered arrays

3. Eliminate the copy to the auxiliary array. Save time (but not space)
by switching the role of the input and auxiliary array in each recursive call




### Bottom-up Mergesort

**Basic Logic**

  * Pass through array, merging subarrays of size 1.
  * Repeat for subarrays of size 2, 4, 8, 16, ....
  * Bottom lin. No recursion needed.
```java
  public class MergeBU
  {
    private static void merge(...)
    { /* as before */ }
    public static void sort(Comparable[] a)
    {
      int N = a.length;
      Comparable[] aux = new Comparable[N];
      for (int sz = 1; sz < N; sz = sz+sz)
      for (int lo = 0; lo < N-sz; lo += sz+sz)
      merge(a, aux, lo, lo+sz-1, Math.min(lo+sz+sz-1, N-1));
    }
  }
```

### Complexity of Sorting

**Computational complexity.** Framework to study efficiency of algorithms for solving a particular problem X.

**Model of computation.** Allowable operations.
**Cost model.** Operation count(s).
**Upper bound.** Cost guarantee provided by some algorithm for X.
**Lower bound.** Proven limit on cost guarantee of all algorithms for X.
**Optimal algorithm.** Algorithm with best possible cost guarantee for X.

Example: sorting.
  * Model of computation: decision tree.(can access information only through compares (e.g., Java Comparable framework))
  * Cost model: # compares.
  * Upper bound: ~ N lg N from mergesort.
  * Lower bound: ~ N lg N
  * Optimal algorithm: = mergesort

### Complexity results in context

Compares? Mergesort is optimal with respect to number compares.
Space? Mergesort is not optimal with respect to space usage.

* Lower bound may not hold if the algorithm has information about:
  * initial order of the input.
  * The distribution of key values.
  * The representation of the keys.

* Partially-ordered arrays. Depending on the initial order of the input, we may not need N lg N compares. 
 => insertion sort requires only N-1 compares if input array is sorted

* Duplicate keys. Depending on the input distribution of duplicates, we may not need N lg N compares.
 => 3-way quicksort
* Digital properties of keys. We can use digit/character compares instead of key compares for numbers and strings.
 => radix sorts

### Comparators

* Compatable interface
* Compatator interface

### Stability

A stable sort preserves the relative order of items with equal keys.

Q. Which sorts are stable?

A. Insertion sort and mergesort (but not selection sort or shellsort).

* Proposition. Insertion sort is stable.
  Pf. Equal items never move past each other.

* Proposition. Selection sort is not stable.
  Pf by counterexample. Long-distance exchange might move an item past some equal item

* Proposition. Shellsort sort is not stable.
  Pf by counterexample. Long-distance exchanges.

* Proposition. Mergesort is stable.
  Pf. Suffices to verify that merge operation is stable.

* Proposition. Merge operation is stable
  Pf. Takes from left subarray if equal keys.


## QuickSort

**Basic plan.**
* Shuffle the array.
* Partition so that, for some j
  * entry a[j] is in place
  * no larger entry to the left of j
  * no smaller entry to the right of j
* Sort each piece recursively.

![quicksort-overview](/assets/quicksort-overview.png)

### Partitioning

The crux of the method is the partitioning process, which rearranges the array to make the following three conditions hold:
* The entry a[j] is in its final place in the array, for some j.
* No entry in a[lo] through a[j-1] is greater than a[j].
* No entry in a[j+1] through a[hi] is less than a[j].

Phase 1: Repeat until i and j pointers cross.
* Scan i from left to right so long as (a[i] < a[lo]).
* Scan j from right to left so long as (a[j] > a[lo]).
* Exchange a[i] with a[j].

Phase 2: When pointers cross.
* Exchange a[lo] with a[j].

### Implementation

Partitioning in-place. Using an extra array makes partitioning easier (and stable), but is not worth the cost.

Terminating the loop. Testing whether the pointers cross is a bit trickier than it might seem.

Staying in bounds. The (j == lo) test is redundant (why?), but the (i == hi) test is not.

Preserving randomness. Shuffling is needed for performance guarantee.

Equal keys. When duplicates are present, it is (counter-intuitively) better to stop on keys equal to the partitioning item's key.

### Empirical Analysis

Best case. Number of compares is ~ N lg N.

Worst case. Number of compares is ~ ½ N^2

Proposition. The average number of compares CN to quicksort an array of
N distinct keys is ~ 2N lnN (and the number of exchanges is ~ ⅓ N ln N).


## Priority Queues

* Collections. Insert and delete items. Which item to delete?
* Stack. Remove the item most recently added.
* Queue. Remove the item least recently added.
* Randomized queue. Remove a random item.
* Priority queue. Remove the largest (or smallest) item.

### Priority queue API

Requirement. Generic items are Comparable

```java
public class MaxPQ< Key extends Comparable<Key>>
MaxPQ() // create a priority queue
MaxPQ(int max) // create a priority queue of initial capacity max
MaxPQ(Key[] a) // create a priority queue from the keys in a[]
void insert(Key v) // insert a key into the priority queue
Key max() // return the largest key
Key delMax() // return and remove the largest key
boolean isEmpty() // is the priority queue empty?
int size() // number of keys in the priority queue
```

#### Priority queue Client Example

**Challenge.** Find the largest M items in a stream of N items. (N huge, M large)
  * Fraud detection: isolate $$ transactions.
  * File maintenance: find biggest files or directories.

**Constraint.** Not enough memory to store N items

**Challenge.** Find the largest M items in a stream of N items.

order of growth of finding the largest M in a stream of N items
| implementation | time | space |
| --- | --- | --- |
| sort | N log N | N |
| elementary PQ | M N | M |
| binary heap | N log M | M |
| best in theory | N | M |


**Challenge.** Implement all operations efficiently.

order of growth of running time for priority queue with N items
| implementation | insert | del | max |
| --- | --- | --- | ---|
| unordered array | 1 | N | N |
| ordered array | N | 1 | 1 |
| goal | log N | log N | log N |

## Binary Heaps

### Complete Binary Tree

**Binary tree.** Empty or node with links to left and right binary trees.

**Complete tree.** Perfectly balanced, except for bottom level.

**Property.** Height of complete tree with N nodes is ⎣lg N⎦.

Pf. Height only increases when N is a power of 2.

### Binary Heep Representation

**Binary heap.** Array representation of a heap-ordered complete binary tree.
**Heap-ordered binary tree.**
  * Keys in nodes.
  * Parent's key no smaller than children's keys.
**Array representation.**
  * Indices start at 1.
  * Take nodes in level order.
  * No explicit links needed!

### Binary Heap Properties

**Proposition.** Largest key is a[1], which is root of binary tree.
**Proposition.** Can use array indices to move through tree.
  * Parent of node at k is at k/2.
  * Children of node at k are at 2k and 2k+1.


## Heapsort 

### Basic 
Basic plan for in-place sort.
  * Create max-heap with all N keys.
  * Repeatedly remove the maximum key.

* **Heap Construction.** Build max heap using bottom-up method. 
* **Sort down.** Repeatedly delete the largest remaining item.
* **First pass.** Build heap using bottom-up method. 
  ```java
  for (int k = N/2; k >= 1; k--)
    sink(a, k, N);
  ```
* **Second pass.** 
  * Remove the maximum, one at a time.
  * Leave in array, instead if nulling out.
  ```java
  while (N > 1)
  {
    exch(a, 1, N--);
    sink(a, 1, N);
  }
  ```

  ```java
  public class Heap
  {
    public static void sort(Comparable[] a)
    {
      int N = a.length;
      for (int k = N/2; k >= 1; k--)
      sink(a, k, N);
      while (N > 1)
      {
        exch(a, 1, N);
        sink(a, 1, --N);
      }
    }
    private static void sink(Comparable[] a, int k, int N)
    { /* as before */ }

    private static boolean less(Comparable[] a, int i, int j)
    { /* as before */ }

    private static void exch(Comparable[] a, int i, int j)
    { /* as before */ }
  }
  ```
### Mathematical Analysis

#### Proposition. 
Heap construction uses ≤ 2 N compares and exchanges.
#### Proposition. 
Heapsort uses ≤ 2 N lg N compares and exchanges.
#### Significance. In-place sorting algorithm with N log N worst-case.
  * Mergesort: no, linear extra space. (in-place merge possible, not practical)
  * Quicksort: no, quadratic time in worst case. (N log N worst-case quicksort possible, not practical)
  * Heapsort: yes!

#### Bottom line. Heapsort is optimal for both time and space, but:
  * Inner loop longer than quicksort’s.
  * Makes poor use of cache memory.
  * Not stable.


## Sorting Algorithms Summary

| ---  | inplace? | stable? | worst | average | best | remarks |
| ---  | ---      | ---     | ---   |  ---    | ---  | ---     |
| selection | x   |         | N^2 / 2 | N^2 / 2 | N^2 / 2 |  N exchanges | 
| insertion | x   |  x      | N^2 / 2 | N^2 / 2 | N^2 / 2 |use for small N or partially ordered |
| shell   |  x   |      |    ?    |    ?    |  N   |  tight code, subquadratic |
| quick   |  x   |      | N^2 / 2 | 2N ln N | N lg N  | N log N probabilistic guarantee fastest in practice |
| 3-way quick |  x  |   | N^2 / 2 | 2N ln N |     N   | improves quicksort in presence of duplicate keys |
| merge |     |  x      | N lg N  | N lg N  | N lg N  | N log N guarantee, stablel |
| heap |  x   |         | 2N lg N | 2N lg N | N lg N  | N log N guarantee, in-place |
| ??? |  x   |  x       | N lgN   | N lg N  | N lg N  | holy sorting grail |



